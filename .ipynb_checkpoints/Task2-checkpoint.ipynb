{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d32521c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/3class/train.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m data \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdev\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m----> 6\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mData/3class/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mname\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m filedata:\n\u001B[0;32m      7\u001B[0m         data[name] \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(filedata)\n",
      "File \u001B[1;32mc:\\users\\colin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    279\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    282\u001B[0m     )\n\u001B[1;32m--> 284\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Data/3class/train.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = {}\n",
    "\n",
    "for name in [\"train\", \"dev\", \"test\"]:\n",
    "    with open(f\"Data/3class/{name}.json\", \"r\") as filedata:\n",
    "        data[name] = json.load(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa98070",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this cell has 2 functions, one that checks language and one that translates if English\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "# recognizes the language of the text and translates if English\n",
    "def recognizeText(text):\n",
    "    if detect(text) != 'no':\n",
    "        return translateText(text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# translates the text given\n",
    "def translateText(text):\n",
    "    translated = translator.translate(text, src='en', dest='no')\n",
    "    return translated.text\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this cell has 2 functions, one that tokenizes the words for each sentence and one that lemmatizes the words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('norwegian'))\n",
    "\n",
    "# function that 'cleans' the text from the sets, tokenizes and removes stop words and punctuations\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in punctuation]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    text = clean_text(text)\n",
    "    return lemmatizer.lemmatize(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_train = []\n",
    "for row in data[\"train\"]:\n",
    "    text = recognizeText(row[\"text\"])\n",
    "    text = lemmatize(text)\n",
    "    text_train.append(text)\n",
    "\n",
    "text_test = []\n",
    "for row in data[\"test\"]:\n",
    "    text = recognizeText(row[\"text\"])\n",
    "    text_test.append(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_label_list = []\n",
    "for row in data[\"train\"]:\n",
    "    train_label_list.append(row[\"label\"])\n",
    "    \n",
    "test_label_list = []\n",
    "for row in data[\"test\"]:\n",
    "    test_label_list.append(row[\"label\"])\n",
    "    \n",
    "y_train = np.array(train_label_list)\n",
    "y_test = np.array(test_label_list)\n",
    "\n",
    "print(f\"Training label shape: {y_train.shape}\")\n",
    "print(f\"Testing label shape: {y_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e23b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "s_words = stopwords.words(\"norwegian\")\n",
    "print(\"Number of Norwegian stopswords: \" + str(len(s_words)))\n",
    "print(f\"LIST OF STOPWORDS: {s_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e3100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(stop_words=s_words, min_df=2)\n",
    "vec.fit(text_train)\n",
    "X_train = vec.transform(text_train).toarray()\n",
    "X_test = vec.transform(text_test).toarray()\n",
    "\n",
    "print(f\"Vocabulary size: {len(vec.vocabulary_)}\")\n",
    "print(f\"Every 100th word in vocabulary: {vec.get_feature_names_out()[::100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17280c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter=75, n_jobs=-1)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4aa8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_pred_1 = logreg.predict(X_train)\n",
    "test_pred_1 = logreg.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression first try scores\")\n",
    "print(\"Training score: {:.3f}\".format(accuracy_score(y_train, train_pred_1)))\n",
    "print(\"Test score: {:.3f}\".format(accuracy_score(y_test, test_pred_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dccd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = {\"Neutral\": 0, \"Positive\": 0, \"Negative\": 0}\n",
    "for label in train_pred_1:\n",
    "    count_dict[label] += 1\n",
    "    \n",
    "display(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=400, n_jobs=-1)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af15678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_2 = rfc.predict(X_train)\n",
    "test_pred_2 = rfc.predict(X_test)\n",
    "\n",
    "print(\"Random Forest scores\")\n",
    "print(\"Training score: {:.3f}\".format(accuracy_score(y_train, train_pred_2)))\n",
    "print(\"Test score: {:.3f}\".format(accuracy_score(y_test, test_pred_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a840881",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 0\n",
    "sum_depth = 0\n",
    "\n",
    "for e in rfc.estimators_:\n",
    "    sum_depth += e.tree_.max_depth\n",
    "    if(e.tree_.max_depth > max_depth):\n",
    "        max_depth = e.tree_.max_depth\n",
    "\n",
    "print(f\"Maximum depth is: {max_depth}\")\n",
    "print(f\"Average depth is: {(sum_depth/len(rfc.estimators_))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffdfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30190070",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_3 = mnb.predict(X_train)\n",
    "test_pred_3 = mnb.predict(X_test)\n",
    "\n",
    "print(\"Multinomial Naive Bayes scores\")\n",
    "print(\"Training score: {:.3f}\".format(accuracy_score(y_train, train_pred_3)))\n",
    "print(\"Test score: {:.3f}\".format(accuracy_score(y_test, test_pred_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f0f049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}